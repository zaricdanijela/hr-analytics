{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data science internship task\n",
    "## Data preparation\n",
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "enrollee_id                  0\n",
       "city                         0\n",
       "city_development_index       0\n",
       "gender                    4508\n",
       "relevent_experience          0\n",
       "enrolled_university        386\n",
       "education_level            460\n",
       "major_discipline          2813\n",
       "experience                  65\n",
       "company_size              5938\n",
       "company_type              6140\n",
       "last_new_job               423\n",
       "training_hours               0\n",
       "target                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "#  load dataset\n",
    "df_tr = pd.read_csv('aug_train.csv')\n",
    "df_te = pd.read_csv('aug_test.csv')\n",
    "\n",
    "#  information about given data\n",
    "df_tr.head()\n",
    "df_tr.shape\n",
    "\n",
    "#  checking null values in given train dataset\n",
    "df_tr.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping rows with missing values would significantly reduce the dataset and underfitting could occur - too many missing values Demonstrating a few other ways to handle with missing values:  fill missing values with 0, fill missing values with mode, fill missing values with median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['company_size']=[1 if company_size==\"<10\" \n",
    "                       else 2 if company_size==\"10/49\"\n",
    "                       else 3 if company_size==\"50-99\"\n",
    "                       else 4 if company_size==\"100-500\"        #do this manually to explicitly tell the model how to group company_size\n",
    "                       else 5 if company_size==\"500-999\"\n",
    "                       else 6 if company_size==\"1000-4999\"\n",
    "                       else 7 if company_size==\"5000-9999\"\n",
    "                       else 8 if company_size==\"10000+\"\n",
    "                       else 0 for company_size in df_tr[\"company_size\"]]\n",
    "\n",
    "experience_new = {'<1': 0, '>20': 21}                           #do this manually to explicitly tell the model how to group experience_new\n",
    "df_tr = df_tr.replace({\"experience\": experience_new})           \n",
    "\n",
    "#  median is used if the data comprised of outliers or more frequent value, we can see that from boxplot\n",
    "df_tr['experience'].fillna(df_tr['experience'].median(), inplace=True)\n",
    "\n",
    "df_tr['last_new_job'] = df_tr['last_new_job'].fillna(\"NA\")\n",
    "lnj = {'>4': 5, 'never': 6}                                     #do this manually to explicitly tell the model how to group last_new_job\n",
    "df_tr = df_tr.replace({\"last_new_job\": lnj})\n",
    "\n",
    "df_tr['gender'] = df_tr['gender'].fillna(\"NA\")                  #missing values fill with NA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values fill with mode because mode is used when the data having more occurences of a particular value or more frequent value. This method is used for the categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr['company_type'].fillna(df_tr['company_type'].mode()[0], inplace=True)         \n",
    "df_tr['major_discipline'].fillna(df_tr['major_discipline'].mode()[0], inplace=True)       \n",
    "df_tr['education_level'].fillna(df_tr['education_level'].mode()[0], inplace=True)\n",
    "df_tr['enrolled_university'].fillna(df_tr['enrolled_university'].mode()[0], inplace=True)\n",
    "\n",
    "#  the same procedure for test dataset\n",
    "df_te.isnull().sum()\n",
    "\n",
    "df_te['company_size']=[1 if company_size==\"<10\"\n",
    "                       else 2 if company_size==\"10/49\"\n",
    "                       else 3 if company_size==\"50-99\"\n",
    "                       else 4 if company_size==\"100-500\"\n",
    "                       else 5 if company_size==\"500-999\"\n",
    "                       else 6 if company_size==\"1000-4999\"\n",
    "                       else 7 if company_size==\"5000-9999\"\n",
    "                       else 8 if company_size==\"10000+\"\n",
    "                       else 0 for company_size in df_te[\"company_size\"]]\n",
    "\n",
    "experience_new = {'<1': 0, '>20': 21}\n",
    "df_te = df_te.replace({\"experience\": experience_new})\n",
    "df_te['experience'].fillna(df_te['experience'].median(), inplace=True)\n",
    "\n",
    "df_te['last_new_job'] = df_te['last_new_job'].fillna(\"NA\")\n",
    "lnj = {'>4': 5, 'never': 6}\n",
    "df_te = df_te.replace({\"last_new_job\": lnj})\n",
    "\n",
    "df_te['gender'] = df_te['gender'].fillna(\"NA\")\n",
    "df_te['company_type'].fillna(df_te['company_type'].mode()[0], inplace=True)\n",
    "df_te['major_discipline'].fillna(df_te['major_discipline'].mode()[0], inplace=True)\n",
    "df_te['education_level'].fillna(df_te['education_level'].mode()[0], inplace=True)\n",
    "df_te['enrolled_university'].fillna(df_te['enrolled_university'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kKzBBwcqmh4U"
   },
   "outputs": [],
   "source": [
    "#  Distribution of target - 75% candidates are not searching for new job\n",
    "plt.figure(1)\n",
    "\n",
    "bars = df_tr['target'].value_counts()\n",
    "graph = plt.bar(['Not searching for new job', 'Searching for new job'], bars.values)\n",
    "plt.title('Distribution of target')\n",
    "plt.xlabel('Target values')\n",
    "plt.ylabel('Count')\n",
    "i = 0\n",
    "for p in graph:\n",
    "    w = p.get_width()\n",
    "    h = p.get_height()\n",
    "    x, y = p.get_xy()\n",
    "    plt.text(x+w/2,y+h*1.01+1,str(round(100*bars[i]/df_tr['target'].count()))+'%', ha = 'center')\n",
    "    i += 1\n",
    "\n",
    "plt.close() #comment this line if you want to show figure 1\n",
    "\n",
    "#  The top 4 cities represent more than half of the candidates\n",
    "plt.figure(2)\n",
    "\n",
    "city = df_tr['city'].value_counts().head(5)\n",
    "graph = plt.bar([str(i) for i in city.keys()], city.values,color = 'red')\n",
    "plt.title('Cities with the most candidates (Top 5)')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Count')\n",
    "i = 0\n",
    "for p in graph:\n",
    "     w = p.get_width()\n",
    "     h = p.get_height()\n",
    "     x, y = p.get_xy()\n",
    "     plt.text(x+w/2,y+h*1.01+1,str(round(100*city.values[i]/df_tr['target'].count()))+'%',ha = 'center')\n",
    "     i += 1\n",
    "\n",
    "plt.close() #comment this line if you want to show figure 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Males make up a majority of participants\n",
    "graph = go.Figure(go.Pie(labels = df_tr['gender'].value_counts().keys().to_list(),values = df_tr['gender'].value_counts().to_list(),hole = 0.5))\n",
    "#graph.update_layout(title_text='Gender distribution', title_x=0.5) #uncomment this line if you want to show figure 3\n",
    "\n",
    "#  The percentage of people looking for a new job is almost the same for all genders. \n",
    "#  Gender has no effect on changing the proportion of job seekers\n",
    "graph = px.histogram(data_frame = df_tr, x = 'gender', color = 'target', title = \"Relationship between target and gender\")\n",
    "#graph.show() #uncomment this line if you want to show figure 4\n",
    "\n",
    "#  As education level increases, the percentage of females also increases \n",
    "graph=px.histogram(data_frame = df_tr, x = 'education_level', color = 'gender', title = \"Gender education level\")\n",
    "#graph.show() #uncomment this line if you want to show figure 5\n",
    "\n",
    "#  The higher education level means the greater amount of candidates with relevant experience\n",
    "graph=px.histogram(data_frame = df_tr, x = 'education_level', color = 'relevent_experience', title = \"Relationship between education level and relevent experience\")\n",
    "#graph.show() #uncomment this line if you want to show figure 6\n",
    "\n",
    "#  STEM is the most common discipline for data scientists\n",
    "graph = go.Figure(go.Pie(labels = df_tr['major_discipline'].value_counts().keys().to_list(),values = df_tr['major_discipline'].value_counts().to_list(),hole = 0.5))\n",
    "#graph.update_layout(title_text='Major discipline', title_x=0.5) #uncomment this line if you want to show figure 7 \n",
    "\n",
    "#  Most of the candidates have more than 20 years of experience\n",
    "graph = px.histogram(data_frame = df_tr, y = 'experience', title = \"Experience\")\n",
    "#graph.show() #uncomment this line if you want to show figure 8\n",
    "\n",
    "#  The less work experience, the probability that the candidate is searching \n",
    "#  for a new job is higher\n",
    "graph = px.histogram(data_frame = df_tr, x = 'experience', color = 'target', title = \"Relationship between experience and target\")\n",
    "#graph.show() #uncomment this line if you want to show figure 9\n",
    "\n",
    "#  The proportion of candidates looking for a new job is consistent across all company sizes\n",
    "graph = px.histogram(data_frame = df_tr, x = 'company_size', color = 'target', title = \"Relationship between company size and target\")\n",
    "#graph.show() #uncomment this line if you want to show figure 10\n",
    "\n",
    "#  Strong relationship between city and city development index\n",
    "graph = px.histogram(data_frame = df_tr, x = 'city', color = 'target', title = \"Relationship between city and target\")\n",
    "#graph.show() #uncomment this line if you want to show figure 11\n",
    "graph = px.histogram(data_frame = df_tr, x = 'city_development_index', color = 'target', title = \"Relationship between city development index and target\")\n",
    "#graph.show() #uncomment this line if you want to show figure 12\n",
    "\n",
    "#  The percentage of people looking for a new job is almost the same for all education levels\n",
    "graph = px.histogram(data_frame = df_tr, x = 'education_level', color = 'target', title = \"Relationship between education level and target\")\n",
    "#graph.show() #uncomment this line if you want to show figure 13\n",
    "\n",
    "#  Similar distribution for both types of candidates \n",
    "graph = px.histogram(data_frame = df_tr, x = 'last_new_job', color = 'target', title = \"Relationship between last new job and target\")\n",
    "#graph.show() #uncomment this line if you want to show figure 14\n",
    "\n",
    "#  The distribution of job seekers is equal  \n",
    "graph = px.histogram(data_frame = df_tr, x = 'relevent_experience', color = 'target', title = \"Relationship between relevant experience and target\")\n",
    "#graph.show() #uncomment this line if you want to show figure 15\n",
    "\n",
    "#  The distribution of job seekers is equal  \n",
    "graph = px.histogram(data_frame = df_tr, x = 'major_discipline', color = 'target', title = \"Relationship between major discipline and target\")\n",
    "#graph.show() #uncomment this line if you want to show figure 16\n",
    "\n",
    "#  Strong connection between variables \n",
    "graph = px.histogram(data_frame = df_tr, x = 'enrolled_university', color = 'target', title = \"Relationship between enrolled university and target\")\n",
    "#graph.show() #uncomment this line if you want to show figure 17\n",
    "\n",
    "#  Strong connection between variables \n",
    "graph = px.histogram(data_frame = df_tr, x = 'company_type', color = 'target', title = \"Relationship between company type and target\")\n",
    "#graph.show() #uncomment this line if you want to show figure 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Correlation Heatmap\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "sns.heatmap(df_tr.corr(),ax=ax,annot=True)\n",
    "plt.title('Correlation Heatmap', weight='bold',fontsize=10)\n",
    "plt.close() #comment this line if you want to show figure 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold\n",
    "import pickle\n",
    "\n",
    "train_data = df_tr.copy()\n",
    "test_df = df_te.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deleting features from train and test dataset that do not affect prediction. \n",
    "- Deleting city because in EDA we have seen that city and city_develompent_index have a very strong relationship enrollee_id does not affect prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data['enrollee_id']\n",
    "del train_data['city']\n",
    "\n",
    "enrolled_id_test_df = test_df['enrollee_id']\n",
    "del test_df['enrollee_id']\n",
    "del test_df['city']\n",
    "\n",
    "#  converting categorical variables into dummy/indicator variables\n",
    "train_data = pd.get_dummies(train_data)\n",
    "test_df = pd.get_dummies(test_df)\n",
    "\n",
    "X = train_data.drop('target', axis=1)\n",
    "y = train_data['target']\n",
    "\n",
    "#  evaluating the performance of algorithm\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 44)\n",
    "\n",
    "#first model: kNN - kNN makes highly accurate predictions and handles big datasets well\n",
    "knn = KNeighborsClassifier(n_neighbors = 20)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_prediction = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics**: \n",
    "- accuracy - a standard, commonly-used performance metric\n",
    "- recall - summarizes how well the positive class was predicted - good for imbalanced classification because they focus on one class\n",
    "- precision - summarizes the fraction of examples assigned to the positive class that belong to the positive class - good for imbalanced classification because they focus on one class\n",
    "- roc auc - can be optimistic under a severe class imbalance when the number of examples in the minority class is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oP9vsBLJBhzf",
    "outputId": "3b9ccbfd-767e-412a-8428-ef172b948106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Accuracy (%) Recall (%) Precision (%) ROC AUC (%)\n",
      "KNN                        76.36       9.98         54.76       53.68\n",
      "Decision Tree              72.63      45.55         43.45       63.38\n",
      "Random Forest              79.07      40.35         59.62       65.84\n",
      "SMOTE Random Forest        77.57      48.30         53.78       67.58\n"
     ]
    }
   ],
   "source": [
    "accuracy_knn = accuracy_score(y_test, knn_prediction)*100\n",
    "recall_knn = recall_score(y_test, knn_prediction)*100\n",
    "precision_knn = precision_score(y_test, knn_prediction)*100\n",
    "roc_knn = roc_auc_score(y_test, knn_prediction)*100\n",
    "\n",
    "#  second model: dtree - requires less effort for data preparation during\n",
    "#  pre-processing, very intuitive and easy to explain, missing values in the data\n",
    "#  also do not affect the process of building a decision tree to any considerable extent\n",
    "dtree = DecisionTreeClassifier()\n",
    "dtree.fit(X_train, y_train)\n",
    "dtree_prediction = dtree.predict(X_test)\n",
    "\n",
    "#  metrics for the second model\n",
    "accuracy_dtree = accuracy_score(y_test, dtree_prediction)*100\n",
    "recall_dtree = recall_score(y_test, dtree_prediction)*100\n",
    "precision_dtree = precision_score(y_test, dtree_prediction)*100\n",
    "roc_dtree = roc_auc_score(y_test, dtree_prediction)*100\n",
    "\n",
    "#  third model: rfc - works well with both categorical and continuous variables,\n",
    "#  can automatically handle missing values, reduces overfitting problem in decision\n",
    "#  trees and reduces the variance and therefore improves the accuracy\n",
    "rfc = RandomForestClassifier(random_state = 0, n_estimators = 256, criterion = 'gini', max_features = 'auto', max_depth = 16)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_prediction = rfc.predict(X_test)\n",
    "\n",
    "#  metrics for the third model\n",
    "accuracy_rfc = accuracy_score(y_test, rfc_prediction)*100\n",
    "recall_rfc = recall_score(y_test, rfc_prediction)*100\n",
    "precision_rfc = precision_score(y_test, rfc_prediction)*100\n",
    "roc_rfc = roc_auc_score(y_test, rfc_prediction)*100\n",
    "\n",
    "#  improvement of the third model - SMOTE (Synthetic Minority Oversampling Technique)\n",
    "#  oversampling method to solve the imbalance problem\n",
    "sm = SMOTE(random_state = 44)\n",
    "X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "steps = [('over', SMOTE()), ('model', RandomForestClassifier())]\n",
    "SM_rf = Pipeline(steps = steps)\n",
    "SM_rf.fit(X_res, y_res)\n",
    "SM_rf_prediction = SM_rf.predict(X_test)\n",
    "\n",
    "#  metrics for the improvement of the third model\n",
    "accuracy_SM_rf = accuracy_score(y_test, SM_rf_prediction)*100\n",
    "recall_SM_rf = recall_score(y_test, SM_rf_prediction)*100\n",
    "precision_SM_rf = precision_score(y_test, SM_rf_prediction)*100\n",
    "roc_SM_rf = roc_auc_score(y_test, SM_rf_prediction)*100\n",
    "\n",
    "#  view models and metrics in tabular form\n",
    "data = {'Accuracy (%)': [\"{0:.2f}\".format(accuracy_knn), \"{0:.2f}\".format(accuracy_dtree), \"{0:.2f}\".format(accuracy_rfc), \"{0:.2f}\".format(accuracy_SM_rf)], 'Recall (%)': [\"{0:.2f}\".format(recall_knn),\"{0:.2f}\".format(recall_dtree), \"{0:.2f}\".format(recall_rfc), \"{0:.2f}\".format(recall_SM_rf)], 'Precision (%)': [\"{0:.2f}\".format(precision_knn), \"{0:.2f}\".format(precision_dtree), \"{0:.2f}\".format(precision_rfc), \"{0:.2f}\".format(precision_SM_rf)], 'ROC AUC (%)': [\"{0:.2f}\".format(roc_knn), \"{0:.2f}\".format(roc_dtree), \"{0:.2f}\".format(roc_rfc), \"{0:.2f}\".format(roc_SM_rf)]}\n",
    "df = pd.DataFrame(data, index = ['KNN', 'Decision Tree', 'Random Forest', 'SMOTE Random Forest'])\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-tune model using GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The best result after tuning parameters is:  77.49 %\n",
      "The best estimator is:  RandomForestClassifier(max_depth=16, n_estimators=64)\n"
     ]
    }
   ],
   "source": [
    "#  define list of parameters\n",
    "max_depth = [2, 8, 16]\n",
    "n_estimators = [64, 128, 256]\n",
    "\n",
    "#  enables searching over any sequence of parameter settings\n",
    "param_grid = dict(max_depth = max_depth, n_estimators = n_estimators)\n",
    "dfrst = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth)\n",
    "\n",
    "#  the parameters of the estimator used to apply these methods are optimized by \n",
    "#  cross-validated grid-search over a parameter grid\n",
    "grid = GridSearchCV(estimator = dfrst, param_grid = param_grid, cv = 5)\n",
    "grid_results = grid.fit(X_train, y_train)\n",
    "\n",
    "#  print best result\n",
    "print(\"\\nThe best result after tuning parameters is: \", \"{0:.2f}\".format(grid.best_score_*100), \"%\")\n",
    "print(\"The best estimator is: \", grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation - StratifiedKFold \n",
    "Works perfectly well for Imbalanced Data: Each fold in stratified cross-validation will have a representation of data of all classes in the same ratio as in the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation Scores are [0.78392484 0.78209812 0.77818372 0.77655965 0.77812582]\n",
      "Average Cross Validation score: 77.98 %\n"
     ]
    }
   ],
   "source": [
    "stratifiedkf = StratifiedKFold(n_splits = 5)\n",
    "score = cross_val_score(rfc, X, y, cv = stratifiedkf)\n",
    "\n",
    "print(\"\\nCross Validation Scores are {}\".format(score))\n",
    "print(\"Average Cross Validation score: {0:.2f}\".format(score.mean()*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on stratified cross-validation results:\n",
    "The cross validation scores are very similar and consistent. Stratified cv provides a more reliable accuracy evaluation.\n",
    "The best model is SMOTE Random Forest. This model has the highest recall and roc auc scores and high accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Da9HM20147l1",
    "outputId": "b88491c6-d7b2-4f2a-add6-76606193bc80"
   },
   "outputs": [],
   "source": [
    "filename = 'SMOTE_rfc.sav'\n",
    "pickle.dump(dtree, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function which takes employee_id as input and returns predicted target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  exporting test data frame to csv\n",
    "final_results = SM_rf.predict(test_df)\n",
    "final_df = pd.DataFrame(data={'target': final_results }, index=enrolled_id_test_df)\n",
    "final_df.to_csv('final_results.csv')\n",
    "\n",
    "#  function which takes employee_id as input and returns predicted target\n",
    "def get_predicted_target(employee_id):\n",
    "  df = pd.read_csv('final_results.csv', index_col='enrollee_id')\n",
    "  return int(df.loc[employee_id, :][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini demo of function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID =  32403 , target =  0\n",
      "ID =  9858 , target =  1\n",
      "ID =  31806 , target =  0\n",
      "ID =  21465 , target =  1\n",
      "ID =  12994 , target =  1\n",
      "ID =  10856 , target =  1\n"
     ]
    }
   ],
   "source": [
    "employee_ids = [32403, 9858, 31806, 21465, 12994, 10856]\n",
    "for id in employee_ids:\n",
    "  print('ID = ', id, ', target = ', get_predicted_target(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Untitled22.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
